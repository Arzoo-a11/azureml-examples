{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-publish-and-run-using-rest-endpoint.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Publish a Pipeline and Invoke the REST endpoint\n",
        "In this notebook, we will see how we can publish a pipeline and then invoke the REST endpoint.\n",
        "\n",
        "### Initialization Steps"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore, Experiment, Dataset\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)\n",
        "\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.pipeline.core.graph import PipelineParameter\n",
        "\n",
        "print(\"Pipeline SDK-specific imports completed\")\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
        "\n",
        "# Default datastore (Azure blob storage)\n",
        "# def_blob_store = ws.get_default_datastore()\n",
        "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
        "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.20.0\n",
            "Pipeline SDK-specific imports completed\n",
            "opendatasetspmworkspace2\n",
            "opendatasetspmrg\n",
            "eastus2\n",
            "21d8f407-c4c4-452e-87a4-e609bfb86248\n",
            "Blobstore's name: workspaceblobstore\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1618429162484
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Targets\n",
        "#### Retrieve an already attached  Azure Machine Learning Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your cluster.\n",
        "amlcompute_cluster_name = \"cpu-cluster\"\n",
        "\n",
        "found = False\n",
        "# Check if this compute target already exists in the workspace.\n",
        "cts = ws.compute_targets\n",
        "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
        "    found = True\n",
        "    print('Found existing compute target.')\n",
        "    compute_target = cts[amlcompute_cluster_name]\n",
        "    \n",
        "if not found:\n",
        "    print('Creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
        "                                                                #vm_priority = 'lowpriority', # optional\n",
        "                                                                max_nodes = 4)\n",
        "\n",
        "    # Create the cluster.\n",
        "    aml_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
        "    \n",
        "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
        "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
        "    aml_compute.wait_for_completion(show_output = True, timeout_in_minutes = 10)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target.\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1618436520233
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a more detailed view of current Azure Machine Learning Compute status, use get_status()\n",
        "# example: un-comment the following line.\n",
        "# print(aml_compute.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1618433416002
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Pipeline Steps with Inputs and Outputs\n",
        "A step in the pipeline can take [dataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py) as input. This dataset can be a data source that lives in one of the accessible data locations, or intermediate data produced by a previous step in the pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading data to the datastore\n",
        "data_path = def_blob_store.upload_files([\"./20news.pkl\"], target_path=\"20newsgroups\", overwrite=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading an estimated of 1 files\n",
            "Uploading ./20news.pkl\n",
            "Uploaded ./20news.pkl, 1 files out of an estimated total of 1\n",
            "Uploaded 1 files\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1618433422307
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference the data uploaded to blob storage using file dataset\n",
        "# Assign the datasource to blob_input_data variable\n",
        "blob_input_data = Dataset.File.from_files(data_path).as_named_input(\"test_data\")\n",
        "print(\"Dataset created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1618436536079
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define intermediate data using OutputFileDatasetConfig\n",
        "processed_data1 = OutputFileDatasetConfig(name=\"processed_data1\")\n",
        "print(\"Output dataset object created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output dataset object created\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1618436540369
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a Step that consumes a dataset and produces intermediate data.\n",
        "In this step, we define a step that consumes a dataset and produces intermediate data.\n",
        "\n",
        "**Open `train.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** \n",
        "\n",
        "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# trainStep consumes the datasource (Datareference) in the previous step\n",
        "# and produces processed_data1\n",
        "\n",
        "source_directory = \"publish_run_train\"\n",
        "\n",
        "trainStep = PythonScriptStep(\n",
        "    script_name=\"train.py\", \n",
        "        arguments=[\"--input_data\", blob_input_data.as_mount(), \"--output_train\", processed_data1],\n",
        "    compute_target= aml_compute, \n",
        "    source_directory=source_directory\n",
        ")\n",
        "print(\"trainStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainStep created\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1618436550106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a Step that consumes intermediate data and produces intermediate data\n",
        "In this step, we define a step that consumes an intermediate data and produces intermediate data.\n",
        "\n",
        "**Open `extract.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.** "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# extractStep to use the intermediate data produced by trainStep\n",
        "# This step also produces an output processed_data2\n",
        "processed_data2 = OutputFileDatasetConfig(name=\"processed_data2\")\n",
        "source_directory = \"publish_run_extract\"\n",
        "\n",
        "extractStep = PythonScriptStep(\n",
        "    script_name=\"extract.py\",\n",
        "    arguments=[\"--input_extract\", processed_data1.as_input(), \"--output_extract\", processed_data2],\n",
        "    compute_target=aml_compute, \n",
        "    source_directory=source_directory)\n",
        "print(\"extractStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extractStep created\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1618436553493
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a Step that consumes multiple intermediate data and produces intermediate data\n",
        "In this step, we define a step that consumes multiple intermediate data and produces intermediate data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PipelineParameter"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step also has a [PipelineParameter](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.graph.pipelineparameter?view=azure-ml-py) argument that help with calling the REST endpoint of the published pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this later in publishing pipeline\n",
        "pipeline_param = PipelineParameter(name=\"pipeline_arg\", default_value=10)\n",
        "print(\"pipeline parameter created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline parameter created\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1618436557940
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open `compare.py` in the local machine and examine the arguments, inputs, and outputs for the script. That will give you a good sense of why the script argument names used below are important.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Now define compareStep that takes two inputs (both intermediate data), and produce an output\n",
        "processed_data3 = OutputFileDatasetConfig(name=\"processed_data3\")\n",
        "\n",
        "# You can register the output as dataset after job completion\n",
        "processed_data3 = processed_data3.register_on_complete(\"compare_result\")\n",
        "\n",
        "source_directory = \"publish_run_compare\"\n",
        "\n",
        "compareStep = PythonScriptStep(\n",
        "    script_name=\"compare.py\",\n",
        "    arguments=[\"--compare_data1\", processed_data1.as_input(), \"--compare_data2\", processed_data2.as_input(), \"--output_compare\", processed_data3, \"--pipeline_param\", pipeline_param],  \n",
        "    compute_target= aml_compute, \n",
        "    source_directory=source_directory)\n",
        "print(\"compareStep created\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compareStep created\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1618436567924
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline1 = Pipeline(workspace=ws, steps=[compareStep])\n",
        "print (\"Pipeline is built\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline is built\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1618436696567
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run published pipeline\n",
        "### Publish the pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline1 = pipeline1.publish(name=\"My_New_Pipeline\", description=\"My Published Pipeline Description\", continue_on_step_failure=True)\n",
        "published_pipeline1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step compare.py [b584a867][4dd8abd7-a672-42aa-aeed-c60f32710e53], (This step will run and generate new outputs)\n",
            "Created step train.py [c3f35dca][cf5b513e-9d8e-42bf-ba38-35886ad97557], (This step will run and generate new outputs)\n",
            "Created step extract.py [281e8e0d][428bdecc-00de-4c66-a301-362034ef1ae8], (This step will run and generate new outputs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "Pipeline(Name: My_New_Pipeline,\nId: a01a396f-cf92-4c59-8ed4-dcf5979aef18,\nStatus: Active,\nEndpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/a01a396f-cf92-4c59-8ed4-dcf5979aef18)",
            "text/html": "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>My_New_Pipeline</td><td><a href=\"https://ml.azure.com/pipelines/a01a396f-cf92-4c59-8ed4-dcf5979aef18?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\" target=\"_blank\" rel=\"noopener\">a01a396f-cf92-4c59-8ed4-dcf5979aef18</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/a01a396f-cf92-4c59-8ed4-dcf5979aef18\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1618436782680
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the continue_on_step_failure parameter specifies whether the execution of steps in the Pipeline will continue if one step fails. The default value is False, meaning when one step fails, the Pipeline execution will stop, canceling any running steps."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Publish the pipeline from a submitted PipelineRun\n",
        "It is also possible to publish a pipeline from a submitted PipelineRun"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit a pipeline run\n",
        "pipeline_run1 = Experiment(ws, 'Pipeline_experiment').submit(pipeline1)\n",
        "# publish a pipeline from the submitted pipeline run\n",
        "published_pipeline2 = pipeline_run1.publish_pipeline(name=\"My_New_Pipeline2\", description=\"My Published Pipeline Description\", version=\"0.1\", continue_on_step_failure=True)\n",
        "published_pipeline2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted PipelineRun 3f60e1ac-0f42-4527-9247-63c0db12fedd\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/Pipeline_experiment/runs/3f60e1ac-0f42-4527-9247-63c0db12fedd?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "Pipeline(Name: My_New_Pipeline2,\nId: 3aa728fe-fde8-48e7-85a8-ec84caab5288,\nStatus: Active,\nEndpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/3aa728fe-fde8-48e7-85a8-ec84caab5288)",
            "text/html": "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>My_New_Pipeline2</td><td><a href=\"https://ml.azure.com/pipelines/3aa728fe-fde8-48e7-85a8-ec84caab5288?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\" target=\"_blank\" rel=\"noopener\">3aa728fe-fde8-48e7-85a8-ec84caab5288</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/3aa728fe-fde8-48e7-85a8-ec84caab5288\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1618436790538
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get published pipeline\n",
        "\n",
        "You can get the published pipeline using **pipeline id**.\n",
        "\n",
        "To get all the published pipelines for a given workspace(ws): \n",
        "```css\n",
        "all_pub_pipelines = PublishedPipeline.get_all(ws)\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PublishedPipeline\n",
        "\n",
        "pipeline_id = published_pipeline1.id # use your published pipeline id\n",
        "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
        "published_pipeline"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "Pipeline(Name: My_New_Pipeline,\nId: a01a396f-cf92-4c59-8ed4-dcf5979aef18,\nStatus: Active,\nEndpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/a01a396f-cf92-4c59-8ed4-dcf5979aef18)",
            "text/html": "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>My_New_Pipeline</td><td><a href=\"https://ml.azure.com/pipelines/a01a396f-cf92-4c59-8ed4-dcf5979aef18?wsid=/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourcegroups/opendatasetspmrg/workspaces/opendatasetspmworkspace2\" target=\"_blank\" rel=\"noopener\">a01a396f-cf92-4c59-8ed4-dcf5979aef18</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/a01a396f-cf92-4c59-8ed4-dcf5979aef18\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1618436798773
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run published pipeline using its REST endpoint\n",
        "[This notebook](https://aka.ms/pl-restep-auth) shows how to authenticate to AML workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "import requests\n",
        "\n",
        "auth = InteractiveLoginAuthentication()\n",
        "aad_token = auth.get_authentication_header()\n",
        "\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "\n",
        "print(\"You can perform HTTP POST on URL {} to trigger this pipeline\".format(rest_endpoint))\n",
        "\n",
        "# specify the param when running the pipeline\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=aad_token, \n",
        "                         json={\"ExperimentName\": \"My_Pipeline1\",\n",
        "                               \"RunSource\": \"SDK\",\n",
        "                               \"ParameterAssignments\": {\"pipeline_arg\": 45}})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can perform HTTP POST on URL https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/21d8f407-c4c4-452e-87a4-e609bfb86248/resourceGroups/opendatasetspmrg/providers/Microsoft.MachineLearningServices/workspaces/opendatasetspmworkspace2/PipelineRuns/PipelineSubmit/a01a396f-cf92-4c59-8ed4-dcf5979aef18 to trigger this pipeline\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1618436804289
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response.raise_for_status()\n",
        "except Exception:    \n",
        "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
        "                    'Response Code: {}\\n'\n",
        "                    'Headers: {}\\n'\n",
        "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
        "\n",
        "run_id = response.json().get('Id')\n",
        "print('Submitted pipeline run: ', run_id)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted pipeline run:  34b8b506-3f58-4a47-9a1f-77fb15151c08\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1618436810043
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next: Data Transfer\n",
        "The next [notebook](https://aka.ms/pl-data-trans) will showcase data transfer steps between different types of data stores."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "order_index": 3,
    "exclude_from_index": false,
    "task": "Demonstrates the use of Published Pipelines",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "star_tag": [
      "featured"
    ],
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "AML Compute"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "Custom"
    ],
    "category": "tutorial",
    "framework": [
      "Azure ML"
    ],
    "friendly_name": "How to Publish a Pipeline and Invoke the REST endpoint",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}