{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Triton Inference Server locally\n",
    "\n",
    "description: (preview) deploy an image classification model trained on densenet locally via Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (1.0.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already up-to-date: tritonclient in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.19.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from tritonclient) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: python-rapidjson>=0.9.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from tritonclient) (1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install --upgrade tritonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code EF2VYZKJQ to authenticate.\n",
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from src.model_utils import download_triton_models, delete_triton_models\n",
    "\n",
    "prefix = Path(\".\")\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Registering model densenet-onnx-example\n",
      "Model(workspace=Workspace.create(name='default', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='azureml-examples'), name=densenet-onnx-example, id=densenet-onnx-example:1484, version=1484, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = \"models\"\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"densenet-onnx-example\",\n",
    "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading model densenet-onnx-example:1484 to /tmp/azureml_taqrk6ar/densenet-onnx-example/1484\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
      "Logging into Docker registry 0e14976437204610b0f33e3f974544ac.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM 0e14976437204610b0f33e3f974544ac.azurecr.io/azureml/azureml_f48883605024e109cfd4abf348b76179\n",
      " ---> a4e431054899\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 349862673b2f\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1NjA1NzVkLWZhMDYtNGU3ZC05NWZiLWY5NjJlNzRlZmQ3YSIsInJlc291cmNlR3JvdXBOYW1lIjoiYXp1cmVtbC1leGFtcGxlcyIsImFjY291bnROYW1lIjoiZGVmYXVsdCIsIndvcmtzcGFjZUlkIjoiMGUxNDk3NjQtMzcyMC00NjEwLWIwZjMtM2UzZjk3NDU0NGFjIn0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 2cbfa4391b1c\n",
      " ---> 03a071148aa9\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpwhkebd_l.py' /var/azureml-app/main.py\n",
      " ---> Running in fe83f7da00b5\n",
      " ---> 3cd7c18cbc34\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 0af6251f5106\n",
      " ---> 47b8098d7ed1\n",
      "Successfully built 47b8098d7ed1\n",
      "Successfully tagged triton-densenet-onnx-local86565:latest\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:6789\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\n",
    "env = Environment.get(ws, \"AzureML-Triton\").clone(\"AML-Triton-Vision\")\n",
    "\n",
    "for pip_package in [\"pillow\", \"numpy\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "env.environment_variables[\"WORKER_COUNT\"] = \"1\"\n",
    "\n",
    "env.register(ws)\n",
    "\n",
    "my_env = Environment.get(ws, \"AML-Triton-Vision\")\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    # this entry script is where we dispatch a call to the Triton server\n",
    "    source_directory=\"src\",\n",
    "    entry_script=\"score_densenet.py\",\n",
    "    environment=my_env,\n",
    ")\n",
    "\n",
    "config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-01-30T00:36:00,215213000+00:00 - triton/run \n2021-01-30T00:36:00,215178200+00:00 - gunicorn/run \n2021-01-30T00:36:00,221996500+00:00 - rsyslog/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.10 (build <unknown>)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\n2021-01-30T00:36:00,321777900+00:00 - Waiting for Triton server to get ready ...\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\nE0130 00:36:02.368082 13 pinned_memory_manager.cc:192] failed to allocate pinned system memory: CUDA driver version is insufficient for CUDA runtime version\nI0130 00:36:02.372913 13 model_repository_manager.cc:714] loading: bidaf-9:1\nI0130 00:36:02.374010 13 model_repository_manager.cc:714] loading: densenet_onnx:1\nI0130 00:36:02.490702 13 onnxruntime.cc:1712] TRITONBACKEND_Initialize: onnxruntime\nI0130 00:36:02.490785 13 onnxruntime.cc:1725] Triton TRITONBACKEND API version: 1.0\nI0130 00:36:02.490802 13 onnxruntime.cc:1731] 'onnxruntime' TRITONBACKEND API version: 1.0\nI0130 00:36:02.517325 13 onnxruntime.cc:1773] TRITONBACKEND_ModelInitialize: bidaf-9 (version 1)\nI0130 00:36:02.517347 13 onnxruntime.cc:1773] TRITONBACKEND_ModelInitialize: densenet_onnx (version 1)\n2021-01-30 00:36:02.611023500 [W:onnxruntime:, graph.cc:863 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611132200 [W:onnxruntime:, graph.cc:863 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611163300 [W:onnxruntime:, graph.cc:863 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611189500 [W:onnxruntime:, graph.cc:863 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611211600 [W:onnxruntime:, graph.cc:863 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611235400 [W:onnxruntime:, graph.cc:863 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:02.611284500 [W:onnxruntime:, graph.cc:863 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nI0130 00:36:02.966189 13 onnxruntime.cc:1814] TRITONBACKEND_ModelInstanceInitialize: densenet_onnx (device 0)\nI0130 00:36:02.984425 13 onnxruntime.cc:1814] TRITONBACKEND_ModelInstanceInitialize: bidaf-9 (device 0)\n2021-01-30 00:36:03.032695800 [W:onnxruntime:, graph.cc:863 Graph] Initializer Word_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032771400 [W:onnxruntime:, graph.cc:863 Graph] Initializer Char_Embedding appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032790300 [W:onnxruntime:, graph.cc:863 Graph] Initializer __OneFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032810200 [W:onnxruntime:, graph.cc:863 Graph] Initializer __ZeroFloat appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032838900 [W:onnxruntime:, graph.cc:863 Graph] Initializer __ZeroFloat_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032866800 [W:onnxruntime:, graph.cc:863 Graph] Initializer __NegINF_Batch appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n2021-01-30 00:36:03.032881400 [W:onnxruntime:, graph.cc:863 Graph] Initializer _Const_0 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\nI0130 00:36:03.323365 13 model_repository_manager.cc:896] successfully loaded 'bidaf-9' version 1\nI0130 00:36:03.324212 13 model_repository_manager.cc:896] successfully loaded 'densenet_onnx' version 1\nI0130 00:36:03.324404 13 server.cc:141] \n+-------------+-----------------------------------------------------------------+------+\n| Backend     | Config                                                          | Path |\n+-------------+-----------------------------------------------------------------+------+\n| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so | {}   |\n+-------------+-----------------------------------------------------------------+------+\n\nI0130 00:36:03.324509 13 server.cc:184] \n+---------------+---------+--------+\n| Model         | Version | Status |\n+---------------+---------+--------+\n| bidaf-9       | 1       | READY  |\n| densenet_onnx | 1       | READY  |\n+---------------+---------+--------+\n\nI0130 00:36:03.324668 13 tritonserver.cc:1621] \n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n| Option                           | Value                                                                                                                                              |\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n| server_id                        | triton                                                                                                                                             |\n| server_version                   | 2.4.0                                                                                                                                              |\n| server_extensions                | classification sequence model_repository schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n| model_repository_path[0]         | /var/azureml-app/azureml-models/densenet-onnx-example/1484/models/triton                                                                           |\n| model_control_mode               | MODE_NONE                                                                                                                                          |\n| strict_model_config              | 0                                                                                                                                                  |\n| pinned_memory_pool_byte_size     | 268435456                                                                                                                                          |\n| min_supported_compute_capability | 6.0                                                                                                                                                |\n| strict_readiness                 | 1                                                                                                                                                  |\n| exit_timeout                     | 30                                                                                                                                                 |\n+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+\n\nI0130 00:36:03.337782 13 grpc_server.cc:3914] Started GRPCInferenceService at 0.0.0.0:8001\nI0130 00:36:03.339133 13 http_server.cc:2717] Started HTTPService at 0.0.0.0:8000\nI0130 00:36:03.383213 13 http_server.cc:2736] Started Metrics Service at 0.0.0.0:8002\n2021-01-30T00:36:05,349265200+00:00 - Triton server ready, starting workers ...\n[2021-01-30 00:36:05 +0000] [11] [INFO] Starting gunicorn 19.9.0\n[2021-01-30 00:36:05 +0000] [11] [INFO] Listening at: http://0.0.0.0:5001 (11)\n[2021-01-30 00:36:05 +0000] [11] [INFO] Using worker: uvicorn.workers.UvicornWorker\n[2021-01-30 00:36:05 +0000] [11] [INFO] WORKER_COUNT is 1\n[2021-01-30 00:36:05 +0000] [121] [INFO] Booting worker with pid: 121\nSPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2021-01-30 00:36:07,434 | root | INFO | Starting up app insights client\n2021-01-30 00:36:07,434 | root | INFO | Starting up request id generator\n2021-01-30 00:36:07,434 | root | INFO | Starting up app insight hooks\n2021-01-30 00:36:07,434 | root | INFO | Invoking user's init function\n2021-01-30 00:36:07,440 | root | INFO | Users's init has completed successfully\n2021-01-30 00:36:07,441 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\n[2021-01-30 00:36:07 +0000] [121] [INFO] Started server process [121]\n[30/Jan/2021:00:36:07 +0000] Started server process [121]\n[2021-01-30 00:36:07 +0000] [121] [INFO] Waiting for application startup.\n[30/Jan/2021:00:36:07 +0000] Waiting for application startup.\n[2021-01-30 00:36:07 +0000] [121] [INFO] Application startup complete.\n[30/Jan/2021:00:36:07 +0000] Application startup complete.\n\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "84 : PEACOCK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "test_sample = requests.get(\"https://aka.ms/peacock-pic\", allow_redirects=True).content\n",
    "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Container has been successfully cleaned up.\nsuccessfully deleted model: densenet_onnx\nsuccessfully deleted model: bidaf-9\n"
     ]
    }
   ],
   "source": [
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "53514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}