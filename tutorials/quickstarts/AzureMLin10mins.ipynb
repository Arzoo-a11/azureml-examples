{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Tutorial: How to train and deploy your model in Azure Machine Learning in 10 minutes\n",
    "\n",
    "In this tutorial, you learn how to quickly get started with Azure Machine Learning. You will train an image classification model using the [MNIST](https://azure.microsoft.com/services/open-datasets/catalog/mnist/) dataset.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "> * Download a dataset and look at the data\n",
    "> * Train an image classification model and log metrics\n",
    "> * Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import Data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "* Download the MNIST dataset\n",
    "* Display some sample images\n",
    "\n",
    "### Download the MNIST dataset\n",
    "\n",
    "Use Azure Open Datasets to get the raw MNIST data files. [Azure Open Datasets](https://docs.microsoft.com/azure/open-datasets/overview-what-are-open-datasets) are curated public datasets that you can use to add scenario-specific features to machine learning solutions for more accurate models. Each dataset has a corrseponding class, `MNIST` in this case, to retrieve the data in different ways.\n",
    "\n",
    "Follow this [how-to](https://aka.ms/azureml/howto/createdatasets) if you want to learn more about Datasets and how to use them.\n",
    "\n",
    "<font color='red'>To evaluate: alternative to downloading as filedataset is to use MNIST.get_tabular_dataset(), which will allow to drop the utils package. Code to be found [here](https://azure.microsoft.com/en-us/services/open-datasets/catalog/mnist/)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612269010939
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "mnist_file_dataset.download(data_folder, overwrite=True)\n",
    "\n",
    "mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\n",
    "                                                 name='mnist_opendataset',\n",
    "                                                 description='training and test dataset',\n",
    "                                                 create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Take a look at the data\n",
    "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. \n",
    "\n",
    "Note this step requires a `load_data` function that's included in an `utils.py` file. This file is placed in the same folder as this notebook. The `load_data` function simply parses the compressed files into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612295353295
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "import glob\n",
    "\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "X_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "\n",
    "# now let's show some randomly chosen images from the traininng set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Train model and log metrics\n",
    "\n",
    "The model is trained using the code below. You will create a new Experiment in your workspace, and your training runs and metrics will be registered there so that this information is available after you've finished training your model.\n",
    "\n",
    "You will be using the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier from the [SciKit Learn framework](https://scikit-learn.org/) to classify the data.\n",
    "\n",
    "* **Note: The model training take around 1 minute to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612274211899
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create experiment and start logging to a new run in the experiment\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "exp = Experiment(workspace=ws, name='sklearn-mnist')\n",
    "run = exp.start_logging(snapshot_directory=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612192423754
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "reg = 0.5\n",
    "clf = LogisticRegression(C=1.0/reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#make predictions using the test set and calculate the accuracy\n",
    "y_hat = clf.predict(X_test) \n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "run.log('regularization rate', np.float(reg))\n",
    "run.log('accuracy', np.float(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version control your models with the Model Registry\n",
    "\n",
    "You can use model registration to store and version your models in your workspace. Registered models are identified by name and version. Each time you register a model with the same name as an existing one, the registry increments the version. Azure Machine Learning supports any model that can be loaded through Python 3.\n",
    "\n",
    "The code below does:\n",
    "\n",
    "1. Saves the model to disk\n",
    "1. Uploads the model file to the run \n",
    "1. Registers the uploaded model file\n",
    "1. Transitions the run to a completed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612296715620
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "path = 'sklearn_mnist_model.pkl'\n",
    "joblib.dump(value=clf, filename=path)\n",
    "\n",
    "run.upload_file(name=path, path_or_stream=path)\n",
    "\n",
    "model = run.register_model(model_name='sklearn_mnist_model', \n",
    "                                model_path=path,\n",
    "                                description='Mnist handwriting recognition')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model in the model registry \n",
    "\n",
    "You can see the stored model by navigating to **Models** in the left-hand menu bar of Azure Machine Learning Studio. Click on the **sklearn_mnist_model** and you can see the details of the model like the experiement run id that created the model.\n",
    "\n",
    "To evaluate: move this section to the end to prevent user from losing context? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "The next cells deploys the model to an Azure Container Instance so that you can score data in real-time (Azure Machine Learning also provides mechanisms to do batch scoring). A real-time endpoint allows application developers to integrate machine learning into their apps.\n",
    "\n",
    "* **Note: The deployment takes around 3 minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612192631344
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create environment for the deploy\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# to install required packages\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)\n",
    "\n",
    "#create config file\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"MNIST\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict MNIST with sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model(ws, 'sklearn_mnist')\n",
    "\n",
    "\n",
    "myenv = Environment.get(workspace=ws, name=\"tutorial-env\", version=\"1\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "service_name = 'sklearn-mnist-svc-' + str(uuid.uuid4())[:4]\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name=service_name, \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*scoring script*](score.py) file referenced in the code above can be found in the same folder as this notebook, and has two functions:\n",
    "\n",
    "1. an `init` function that executes once when the service starts - in this function you normally get the model from the registry and set global variables\n",
    "1. a `run(data)` function that executes each time a call is made to the service. In this function, you normally deserialize the json, run a prediction and output the predicted result.\n",
    "\n",
    "\n",
    "## Test the model service\n",
    "\n",
    "You can test the model by sending a raw HTTP request to test the web service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612192861620
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# scoring web service HTTP endpoint\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612192866243
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#send raw HTTP request to test the web service.\n",
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# for AKS deployment you'd need to the service key in the header as well\n",
    "# api_key = service.get_key()\n",
    "# headers = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### View the results of your training\n",
    "\n",
    "When you're finished with an experiment run you can always return to view the results of your model training here in the Azure Machine Learning studio:\n",
    "\n",
    "1. Select **Experiments** (left-hand menu)\n",
    "1. Select **sklearn-mnist**\n",
    "1. Select **Run 1**\n",
    "1. Select the **Metrics** Tab\n",
    "\n",
    "The metrics tab will display the parameter values that were logged to the run.\n",
    "\n",
    "### View the model in the model registry\n",
    "\n",
    "You can see the stored model by navigating to **Models** in the left-hand menu bar. Click on the **sklearn_mnist_model** and you can see the details of the model like the experiement run id that created the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control cost and clean up resources\n",
    "\n",
    "To clean up the resources after this quickstart, first you delete the Model service using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612298075945
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#if you want to keep workspace and only delete endpoint (it will incur cost while running)\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to control cost you can also stop the compute instance this notebook is running on by following these steps:\n",
    "\n",
    "1. Go to **Compute** in the left-hand menu of the Azure Machine Learning studio\n",
    "1. Select your compute instance\n",
    "1. Select **Stop**\n",
    "\n",
    "\n",
    "**Important: The resources you created can be used as prerequisites to other Azure Machine Learning tutorials and how-to articles.** If you don't plan to use the resources you created, delete them, so you don't incur any charges:\n",
    "\n",
    "1. In the Azure portal, select **Resource groups** on the far left.\n",
    "1. From the list, select the resource group you created.\n",
    "1. Select **Delete resource group**.\n",
    "1. Enter the resource group name. Then select **Delete**.\n",
    "\n",
    "You can also keep the resource group but delete a single workspace. Display the workspace properties and select **Delete**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial, you have seen how to run your machine learning code in Azure Machine Learning. \n",
    "\n",
    "It is often the case that once you have your machine learning code working in a development environment that you want to productionize this by using the Python SDK and run the code as a **_job_** - ideally on a schedule or trigger (for example, arrival of new data). To this end, we recommend that you follow the next tutorial in this series, [**Getting started with the Python SDK**](GettingStartedWithPythonSDK.ipynb). This 3-part tutorial is focused on running jobs-based machine learning code in the cloud."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
