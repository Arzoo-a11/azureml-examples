{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Spark Job on Synapse Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "source": [
    "## Input data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "file_name = 'Titanic.csv'\n",
    "source = 'https://dprepdata.blob.core.windows.net/demo/{}'.format(file_name)\n",
    "dest = '{}://{}.blob.{}/{}/{}'.format(\n",
    "    datastore.protocol, \n",
    "    datastore.account_name,\n",
    "    datastore.endpoint,\n",
    "    datastore.container_name,\n",
    "    file_name)\n",
    "dest_key = datastore.account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!azcopy --source $source --destination $dest --dest-key $dest_key --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dataset = Dataset.File.from_files(path=[(datastore, file_name)])\n",
    "input = titanic_dataset.as_hdfs()"
   ]
  },
  {
   "source": [
    "## Output Config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "output = HDFSOutputDatasetConfig(destination=(datastore,\"test\")).register_on_complete(name=\"registered_dataset\")"
   ]
  },
  {
   "source": [
    "## dataprep script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/test.py\n",
    "import os\n",
    "import sys\n",
    "import azureml.core\n",
    "from pyspark.sql import SparkSession\n",
    "from azureml.core import Run\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "print(os.environ)\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_dir\")\n",
    "parser.add_argument(\"--output_dir\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "sdf = spark.read.option(\"header\", \"true\").csv(args.input_dir)\n",
    "sdf.show()\n",
    "\n",
    "sdf.coalesce(1).write\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".mode(\"append\")\\\n",
    ".csv(args.output_dir)"
   ]
  },
  {
   "source": [
    "## Submit an Experiment "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import CondaDependencies\n",
    "conda_dep = CondaDependencies()\n",
    "conda_dep.add_pip_package(\"azureml-core==1.20.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import RunConfiguration\n",
    "from azureml.core import ScriptRunConfig \n",
    "from azureml.core import Experiment \n",
    "\n",
    "run_config = RunConfiguration(framework=\"pyspark\")\n",
    "run_config.target = 'synapsecompute'\n",
    "\n",
    "run_config.spark.configuration[\"spark.driver.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
    "run_config.spark.configuration[\"spark.executor.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
    "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
    "\n",
    "\n",
    "run_config.environment.python.conda_dependencies = conda_dep\n",
    "\n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory = './code',\n",
    "                                    script= 'test.py',\n",
    "                                    arguments = [\"--input_dir\", input, \"--output_dir\", output],\n",
    "                                    run_config = run_config) \n",
    "\n",
    "\n",
    "\n",
    "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
    "run = exp.submit(config=script_run_config) \n",
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}